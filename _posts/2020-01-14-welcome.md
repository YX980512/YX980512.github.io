# Diffusion Model Learning Notes

Here's the table of contents:

1. TOC
{:toc}

# Diffusion Model Learning Notes

## Fundamentals of diffusion models
The Diffusion Model is a type of generative model, which, unlike other generative networks such as VAEs (Variational Autoencoders) and GANs (Generative Adversarial Networks), gradually applies noise to an image in the forward phase until the image is degraded into pure Gaussian noise. Then, in the reverse phase, it learns the process of restoring the Gaussian noise back to the original image.

![](/images/img_dif.jpg "image generated by DF")





## Initial Condition and Prediction

![](/images/diffusion.png "diffusion")

- Initial state of the system $x_0$
- Discretized time step $t$
- The disturbance $\epsilon_t \sim \mathcal{N}(0, I)$
- The prediction error $\epsilon_0 (\sqrt{\alpha_t}x_0 + \sqrt{1 - \alpha_t}\epsilon_t)$
- The estimated mean squared error (MSE) Loss: $\|e_t - \epsilon_0 (\sqrt{\alpha_t}x_0 + \sqrt{1 - \alpha_t}\epsilon_t)\|^2$, the goal is to minimize this loss.

## Reverse Process for Sampling from the Learned Distribution

- Final state of the system $x_T$
- For $T, ..., 1$ do reverse time steps:
  - For $t = 1$ set $z = 0$; for $t > 1$, sample $z_t$ from $\mathcal{N}(0, I)$
  - Update $x_{t-1}$ from $x_t$ using the reparameterization trick from equation (12), where:
    $\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1-\alpha_t}} \epsilon_\theta(x_t, t) \right)$
    and $\theta$ represents the learned parameters.
  - Update the standard deviation as $\sigma_t = \sqrt{\beta_t} = \sqrt{\frac{1-\alpha_t}{1-\bar{\alpha}_t} \beta_t}$
  - The new state is sampled as $x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z$

- Finally, we get the initial state, the starting point $x_0$.
